# Classification-Model-for-ASL
The objective of this research project is to design and develop a highly accurate real-time classification model capable of recognizing and interpreting American Sign Language (ASL) gestures from video data. ASL, a visual language used by the deaf and hard-of-hearing community in the United States, can be better understood by a machine learning model, thereby improving communication accessibility for this community.


##### KEYWORDS
Classification Model, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Image Processing, Spatial Features, Temporal Features.


### Data
The ASL Alphabet dataset contains over 87,000 labeled images of hand gestures representing each letter of the American Sign Language (ASL) alphabet. The dataset is split into two parts: a training set containing 64,800 images and a test set containing 26,400 images. 

![ASL DataSet](https://github.com/Ashleshk/Classification-Model-for-ASL/blob/main/images/data.png)

