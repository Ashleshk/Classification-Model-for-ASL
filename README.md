# Classification-Model-for-ASL
The objective of this research project is to design and develop a highly accurate real-time classification model capable of recognizing and interpreting American Sign Language (ASL) gestures from video data. ASL, a visual language used by the deaf and hard-of-hearing community in the United States, can be better understood by a machine learning model, thereby improving communication accessibility for this community.

This research project aims to address these limitations by developing a more sophisticated and robust model that can generalize well to different signers, signing styles, and environments. The proposed model will be designed to capture not only the spatial and temporal features of the video data but also the context and meaning of the signs within a broader linguistic context.

##### KEYWORDS
Classification Model, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Image Processing, Spatial Features, Temporal Features.


### Data
The ASL Alphabet dataset contains over 87,000 labeled images of hand gestures representing each letter of the American Sign Language (ASL) alphabet. The dataset is split into two parts: a training set containing 64,800 images and a test set containing 26,400 images. 


![ASL DataSet](https://github.com/Ashleshk/Classification-Model-for-ASL/blob/main/images/data.png)

